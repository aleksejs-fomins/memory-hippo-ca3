{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Coding\n",
    "\n",
    "Following Rao&Ballard1999, consider a multi-layered recurrent network of rate neurons, where the first layer $I$ represents the input to the system, and the remaining layers $x_i$ are the representations of that input.\n",
    "<img src=\"../../img/predictive-rao-ballard-1.png\" title=\"img1\" style=\"height:80px\">\n",
    "\n",
    "The connections in the above network are not shown, as we will derive them mathematically. The main constraint of R&B is that a good approximation of each representation $x_i$, including the input, can be computed from the previous layer using a matrix multiplication and a nonlinearity (e.g. sigmoid), namely $\\hat{x_i} = f(U_i x_{i+1}) + \\nu$, where $\\nu$ is the possible\n",
    "<img src=\"../../img/predictive-rao-ballard-2.png\" title=\"img2\" style=\"height:100px\">\n",
    "\n",
    "As such, the system must evolve to minimize the error of its predictive power. The global squared error is given by\n",
    "\n",
    "$$E^2\n",
    "= \\sum_i \\epsilon_i^2\n",
    "= \\sum_i |r_{i} - \\hat{r}_{i}|^2\n",
    "= \\sum_i |r_{i} - f(U_i r_{i+1})|^2\n",
    "$$\n",
    "\n",
    "Any given representation $r_i$ has two contributions to the global error: $\\epsilon_{i-1}$ determines the efficiency of the representation $r_i$ to predict the previous layer, and $\\epsilon_{i}$ the efficiency of being predictable by the next layer. The dynamics of each layer can then be computed as via a gradient descent of the error function\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\tau_i \\dot{x}_i\n",
    "&=& -\\frac{1}{2}\\partial_{x_i} E^2 \\\\\n",
    "&=& -\\frac{1}{2}\\partial_{x_i} (\\epsilon_{i-1}^2 +  \\epsilon_{i}^2) \\\\\n",
    "&=& U_{i-1}^T \\epsilon_{i-1} f'(U_{i-1}x_i) - \\epsilon_i \\\\\n",
    "&=& U_{i-1}^T (x_{i-1} - f(U_{i-1}x_i)) f'(U_{i-1}x_i) - (r_{i} - f(U_i x_{i+1})) \\\\\n",
    "&=& -x_{i} + U_{i-1}^T x_{i-1} f'(U_{i-1}x_i) + f(U_i x_{i+1}) - U_{i-1}^T f(U_{i-1}x_i) f'(U_{i-1}x_i)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "For simplicity, choose a linear function $f(x) = x$, then $f'(x) = 1$, and the dynamics equation can be rewritten as\n",
    "\n",
    "$$\\tau_i \\dot{x}_i = -x_{i} + U_{i-1}^T x_{i-1} + U_i x_{i+1} - U_{i-1}^T U_{i-1}x_i$$\n",
    "\n",
    "The terms of the RHS can be identified as leak, forwards-propagated representation, backwards-propagated representation, and self-inhibition. If we introduce the auxiliary inhibitory populations $i_i$, we can now draw the connectivity\n",
    "<img src=\"../../img/predictive-rao-ballard-3.png\" title=\"img3\" style=\"height:150px\">\n",
    "\n",
    "Gradient descent can also be used to compute the plasticity rule for optimizing the representation matrix\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\tau_{U_i} \\dot{U}_i\n",
    "&=& -\\frac{1}{2}\\partial_{U_i} E^2\n",
    "= -\\frac{1}{2}\\partial_{U_i} \\epsilon_{i}^2 \\\\\n",
    "&=& (x_{i} - f(U_{i} x_{i+1})) f'(U_{i}x_{i+1}) x_{i+1}^T \\\\\n",
    "&=& x_{i} x_{i+1}^T f'(U_{i}x_{i+1}) - f(U_{i} x_{i+1})x_{i+1}^T f'(U_{i}x_{i+1})\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Again, assuming $f(x) = x$, we can simplify the plasitcity rule to\n",
    "$$\\tau_{U_i} \\dot{U}_i = x_{i} x_{i+1}^T - U_{i} x_{i+1} x_{i+1}^T$$\n",
    "It is a form of Hebbian plasticity\n",
    "\n",
    "**Criticism**:\n",
    "* Same matrix weights $U_i$ appears 4 times in the design, but plasticity rule only given for the feedback weight.\n",
    "* Feedforward and Feedback connectivity is symmetric.\n",
    "* Connectivity $U_i$ is dense.\n",
    "* The equation for highest layer are different than for the rest, as it does not receive input from above. Thus it does not have a natural leak term. Perhaps a leak term can be extracted by decomposing it from the self-inhibition, but that is only possible if the self-inhibition is at least as strong as the leak would be. Point being, it is not obvious that the equations selected to govern the last layer are consistent with the ones of rate-based neurons.\n",
    "* The above equations do not take into account synaptic lags\n",
    "\n",
    "**TODO**:\n",
    "* Implement exact version of above, see if it really works\n",
    "* Continue with Bogacz2017\n",
    "* Add priors from R&B, understand what they do (low priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inputLib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77f6124c9774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minputLib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenRandomImg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inputLib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from inputLib import genRandomImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Generate some random pictures\n",
    "##################################\n",
    "N_IMAGES = 5\n",
    "NPIX_ROW = 20\n",
    "NPIX_COL = 20\n",
    "\n",
    "images = [genRandomImg(NPIX_ROW, NPIX_COL) for i in range(N_IMAGES)]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=N_IMAGES, figsize=(5*N_IMAGES, 5))\n",
    "for i in range(N_IMAGES):\n",
    "    ax[i].imshow(images[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive implementation - Deep N-Layer Impl\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\tau_{x_1} \\dot{x_1} &=& U_1^T (I - U_1 x_1) -x_1 + U_2 x_2 \\\\\n",
    "  \\tau_{x_i} \\dot{x_i} &=& U_i^T (x_{i-1} - U_i x_i) -x_i + U_{i+1} x_{i+1} \\\\\n",
    "  \\tau_{x_n} \\dot{x_n} &=& U_n^T (x_{n-1} - U_n x_n) \\\\\n",
    "  \\tau_{U_1} \\dot{U_1} &=& (I - U_1 x_1) x_1^T \\\\\n",
    "  \\tau_{U_i} \\dot{U_i} &=& (x_{i-1} - U_i x_i) x_i^T\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "**Conclusions**:\n",
    "* **Problem: Degeneration** - higher levels frequently end up being the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Write down update rules\n",
    "##################################\n",
    "def update(x, U, I, p, WITH_SP=True):\n",
    "    N_LAYER = len(x)\n",
    "    xnew = []\n",
    "    Unew = []\n",
    "    zeta_x = p['dt'] / p['tau_x']\n",
    "    zeta_U = p['dt'] / p['tau_U']\n",
    "    for i in range(N_LAYER):\n",
    "        Ieff = I if i == 0 else x[i-1]\n",
    "        eps = Ieff - U[i].dot(x[i])\n",
    "        \n",
    "        # Add feedforward and feedback input input\n",
    "        RHS_X = U[i].T.dot(eps)\n",
    "        if i != N_LAYER-1:\n",
    "            RHS_X += -x[i] + U[i+1].dot(x[i+1]) \n",
    "        xnew += [x[i] + zeta_x * RHS_X]\n",
    "        \n",
    "        # Synaptic Plasticity\n",
    "        if WITH_SP:\n",
    "            RHS_U = np.outer(eps, x[i])\n",
    "            Unew += [U[i] + zeta_U * RHS_U]\n",
    "        \n",
    "    return (xnew, Unew) if WITH_SP else xnew\n",
    "\n",
    "##################################\n",
    "# Initialize model\n",
    "##################################\n",
    "\n",
    "param = {\n",
    "    'dt' : 0.0001,\n",
    "    'tau_x' : 0.001,\n",
    "    'tau_U' : 0.1,\n",
    "    \"noise_mag\" : 0}\n",
    "\n",
    "N_INP = NPIX_ROW*NPIX_COL\n",
    "INP_lst = [pic.flatten() for pic in images]\n",
    "N_IMG = 5\n",
    "\n",
    "N_LAYER = 2\n",
    "N_X = [3, 3]\n",
    "N_IX = [N_INP] + N_X\n",
    "\n",
    "x = [np.random.uniform(0.9, 1, nx) for nx in N_X] \n",
    "U = [np.random.uniform(0, 1, N_IX[i-1]*N_IX[i]).reshape((N_IX[i-1], N_IX[i])) for i in range(1, N_LAYER+1)]\n",
    "U = [u / np.linalg.norm(u) for u in U]\n",
    "\n",
    "##################################\n",
    "# Training\n",
    "##################################\n",
    "for iStep in range(200000):\n",
    "#     if iStep % 10000 == 0:\n",
    "#         print(iStep)\n",
    "    idx_I = (iStep // 2000) % N_IMG\n",
    "    I_this = INP_lst[idx_I]# + param['noise_mag'] * np.random.normal(0, 1, N_INP)\n",
    "    x, U = update(x, U, I_this, param)\n",
    "    \n",
    "##################################\n",
    "# Testing\n",
    "##################################\n",
    "N_STEP_TEST = 20000\n",
    "times_lst = np.linspace(0, N_STEP_TEST-1, N_STEP_TEST) * param['dt']\n",
    "x_data = []\n",
    "err = np.zeros((N_STEP_TEST, N_LAYER))\n",
    "for iStep in range(N_STEP_TEST):\n",
    "    idx_I = (iStep // 2000) % N_IMG\n",
    "    I_this = INP_lst[idx_I] + param['noise_mag'] * np.random.normal(0, 1, N_INP)\n",
    "    x = update(x, U, I_this, param, WITH_SP=False)\n",
    "    x_data += [x]\n",
    "    \n",
    "    for iLayer in range(N_LAYER):\n",
    "        Ieff = I_this if iLayer == 0 else x[iLayer-1]\n",
    "        err[iStep][iLayer] = np.linalg.norm(Ieff - U[iLayer].dot(x[iLayer])) / np.sqrt(len(Ieff))\n",
    "    \n",
    "##################################\n",
    "# Plotting\n",
    "##################################\n",
    "def backprop_receptive(U, depth, iLayer, iX):\n",
    "    return U[iLayer][:,iX] if depth==0 else U[iLayer].dot(backprop_receptive(U, depth-1, iLayer+1, iX))\n",
    "\n",
    "# Receptive fields\n",
    "for iLayer in range(N_LAYER):\n",
    "    fig, ax = plt.subplots(ncols=N_X[iLayer], figsize=(3*N_X[iLayer], 3))\n",
    "    fig.suptitle(\"Receptive Fields, Layer \"+str(iLayer))\n",
    "    for iX in range(N_X[iLayer]):\n",
    "        ax[iX].imshow(backprop_receptive(U, iLayer, 0, iX).reshape((NPIX_ROW, NPIX_COL)))\n",
    "        ax[iX].set_title(\"X\"+str(iLayer)+str(iX))\n",
    "    plt.show()\n",
    "\n",
    "# Representation values\n",
    "fig, ax = plt.subplots(ncols=N_LAYER, figsize=(5*N_LAYER, 5))\n",
    "fig.suptitle(\"Representation Values\")\n",
    "for iLayer in range(N_LAYER):\n",
    "    xthis = np.array([x[iLayer] for x in x_data])\n",
    "    ax_this = ax[iLayer] if N_LAYER > 1 else ax    # In matlab ax is not a list if it has only one element :(\n",
    "    for iX in range(N_X[iLayer]):\n",
    "        ax_this.plot(times_lst, xthis[:, iX])\n",
    "        ax_this.set_title(\"Layer \"+str(iLayer))\n",
    "plt.show()\n",
    "\n",
    "# Errors\n",
    "plt.figure()\n",
    "plt.title(\"Representation error\")\n",
    "for iLayer in range(N_LAYER):\n",
    "    plt.plot(times_lst, err[:, iLayer], label=\"Layer \"+str(iLayer))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
